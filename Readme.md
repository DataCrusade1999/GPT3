# GPT3

Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text. It is the third-generation language prediction model in the GPT-n series (and the successor to GPT-2) created by OpenAI, a San Francisco-based artificial intelligence research laboratory. GPT-3's full version has a capacity of 175 billion machine learning parameters.

## In this repo I have asked some question to GPT-3 and GPT-3 gave some answer. sometimes the answers are vague and stray off-point and sometimes answers are on point but all this depends on the value of the parameters which I haven't played with yet ðŸ˜‰
